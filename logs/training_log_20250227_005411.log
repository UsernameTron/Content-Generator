2025-02-27 00:54:13,614 - __main__ - INFO - Initializing W&B project: pete-connor-cx-ai-expert
2025-02-27 00:54:13,616 - __main__ - INFO - Loaded environment variables from .env file
2025-02-27 00:54:13,616 - __main__ - INFO - WANDB_API_KEY found, running in online mode
2025-02-27 00:54:14,460 - __main__ - INFO - Successfully initialized W&B
2025-02-27 00:54:14,475 - __main__ - INFO - Using MPS (Metal Performance Shaders) for training on Apple Silicon
2025-02-27 00:54:14,476 - __main__ - INFO - Loading base model: EleutherAI/pythia-1.4b
2025-02-27 00:54:15,023 - __main__ - INFO - Tokenizer loaded successfully
2025-02-27 00:54:15,023 - __main__ - INFO - Disabled FP16 as it's only supported on CUDA devices
2025-02-27 00:54:15,023 - __main__ - INFO - Loading model on mps device
2025-02-27 00:54:17,022 - __main__ - INFO - Model loaded successfully
2025-02-27 00:54:17,022 - __main__ - INFO - Preparing model for kbit training
2025-02-27 00:54:17,022 - __main__ - INFO - Configuring LoRA adapters
/Users/cpconnor/CascadeProjects/multi-platform-content-generator/venv/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
  warn("The installed version of bitsandbytes was compiled without GPU support. "
2025-02-27 00:54:17,104 - __main__ - INFO - LoRA adapters successfully applied to model
2025-02-27 00:54:17,105 - __main__ - INFO - Model size: 1420.94M parameters
2025-02-27 00:54:17,106 - __main__ - INFO - Trainable parameters: 6.29M (0.44%)
2025-02-27 00:54:17,106 - __main__ - INFO - Preparing training dataset from data/training_data.jsonl
2025-02-27 00:54:17,106 - __main__ - INFO - Loading data from data/training_data.jsonl
2025-02-27 00:54:17,158 - __main__ - INFO - Preparing validation dataset from data/training_data.jsonl
2025-02-27 00:54:17,158 - __main__ - INFO - Loading data from data/training_data.jsonl
2025-02-27 00:54:17,208 - __main__ - INFO - Training dataset size: 81
2025-02-27 00:54:17,208 - __main__ - INFO - Evaluation dataset size: 81
2025-02-27 00:54:17,209 - wandb_dashboards - INFO - Created custom metrics callback for W&B tracking
2025-02-27 00:54:17,209 - __main__ - INFO - Custom W&B metrics callback loaded successfully
2025-02-27 00:54:17,210 - __main__ - INFO - Setting up training arguments
/Users/cpconnor/CascadeProjects/multi-platform-content-generator/venv/lib/python3.13/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
2025-02-27 00:54:17,212 - __main__ - INFO - Initializing trainer
/Users/cpconnor/CascadeProjects/multi-platform-content-generator/finetune_model.py:525: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
2025-02-27 00:54:17,216 - __main__ - INFO - Checkpoints will be saved every 7200 steps
2025-02-27 00:54:17,216 - __main__ - INFO - Starting model fine-tuning
'NoneType' object has no attribute 'cadam32bit_grad_fp32'
  0%|          | 0/20 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/Users/cpconnor/CascadeProjects/multi-platform-content-generator/venv/lib/python3.13/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
2025-02-27 00:54:52,221 - __main__ - ERROR - Error during training: MPS backend out of memory (MPS allocated: 10.11 GB, other allocations: 50.66 GB, max allowed: 61.20 GB). Tried to allocate 3.07 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
2025-02-27 00:54:52,244 - __main__ - WARNING - Training was interrupted. Saving checkpoint of the current state.
2025-02-27 00:54:52,251 - __main__ - INFO - Saving model to outputs/finetune/final
2025-02-27 00:54:53,602 - __main__ - INFO - Model saving complete.
2025-02-27 00:54:53,603 - __main__ - ERROR - Error saving model: Object of type set is not JSON serializable
2025-02-27 00:54:53,603 - __main__ - ERROR - Could not save the trained model. Check the logs for details.
2025-02-27 00:55:01,261 - __main__ - INFO - Fine-tuning completed
/Users/cpconnor/CascadeProjects/multi-platform-content-generator/venv/lib/python3.13/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (zj73jbeu) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
  lambda data: self._console_raw_callback("stderr", data),
  0%|          | 0/20 [00:43<?, ?it/s]
=== C. Pete Connor Model Training Log - Started at 2025-02-27T00:54:11.183661 ===



=== Training completed successfully! Finished at 2025-02-27T00:55:02.448590 ===
