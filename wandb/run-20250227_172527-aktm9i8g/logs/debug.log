2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_setup.py:_flush():76] Current SDK version is 0.16.6
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_setup.py:_flush():76] Configure stats pid to 3239
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_setup.py:_flush():76] Loading settings from /Users/cpconnor/.config/wandb/settings
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_setup.py:_flush():76] Loading settings from /Users/cpconnor/CascadeProjects/multi-platform-content-generator/wandb/settings
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'api_key': '***REDACTED***', 'project': 'pete-connor-cx-ai-expert', 'silent': 'true', 'notebook_name': 'finetune_model.py'}
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'finetune_model.py', 'program_abspath': '/Users/cpconnor/CascadeProjects/multi-platform-content-generator/finetune_model.py', 'program': '/Users/cpconnor/CascadeProjects/multi-platform-content-generator/finetune_model.py'}
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_setup.py:_flush():76] Applying login settings: {}
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_init.py:_log_setup():521] Logging user logs to /Users/cpconnor/CascadeProjects/multi-platform-content-generator/wandb/run-20250227_172527-aktm9i8g/logs/debug.log
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_init.py:_log_setup():522] Logging internal logs to /Users/cpconnor/CascadeProjects/multi-platform-content-generator/wandb/run-20250227_172527-aktm9i8g/logs/debug-internal.log
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_init.py:init():561] calling init triggers
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_init.py:init():568] wandb.init called with sweep_config: {}
config: {'base_model': 'EleutherAI/pythia-1.4b', 'training_config': {'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'training_args': {'output_dir': './outputs/finetune', 'evaluation_strategy': 'steps', 'eval_steps': 100, 'logging_steps': 10, 'save_steps': 7200, 'save_total_limit': 5, 'learning_rate': 2e-05, 'weight_decay': 0.01, 'fp16': False, 'bf16': False, 'max_grad_norm': 0.3, 'max_steps': -1, 'num_train_epochs': 10, 'warmup_ratio': 0.03, 'group_by_length': True, 'lr_scheduler_type': 'cosine', 'report_to': 'wandb', 'gradient_checkpointing': True, 'gradient_accumulation_steps': 16, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'run_name': 'overnight-full-training-run'}}, 'data_config': {'train_file': 'data/training_data.jsonl', 'validation_file': 'data/validation_data.jsonl', 'preprocessing_num_workers': 4, 'max_seq_length': 1024, 'overwrite_cache': False, 'pad_to_max_length': True}, 'wandb_config': {'project': 'pete-connor-cx-ai-expert', 'name': 'overnight-full-training-run', 'tags': ['cx-ai-expert', 'customer-experience', 'machine-learning', 'satirical-tech-expert', 'lora-finetuning', 'continuous-training'], 'notes': "Overnight continuous training run for C. Pete Connor's expertise in customer experience, AI, and ML with enhanced anti-pattern training"}, 'custom_loss_config': {'penalized_phrases': ['game changer', "here's the kicker", 'cutting-edge', 'revolutionary', 'disruptive', 'innovative', 'next-generation', 'state-of-the-art', 'seamless customer journey', 'delightful experience', 'customer-centric', 'AI-powered experience', 'frictionless', 'hyper-personalization', 'digital transformation', 'customer obsession', 'paradigm shift', 'market disruption', 'synergy', 'leverage', 'holistic approach', 'seamless integration', 'robust solution', 'scalable architecture', 'best-in-class', 'end-to-end solution', 'turnkey solution', 'low-hanging fruit', 'think outside the box', 'moving the needle', 'overuse of em dash', 'overuse of en dash', 'artificial transitions', 'symmetric sentence structures'], 'rewarded_phrases': ['paradoxically', 'absurdly', 'statistically speaking', 'data shows', 'ironically', 'contrary to popular belief', 'in stark contrast to the marketing', 'customer experience data indicates', 'model bias reveals', 'sentiment analysis demonstrates', 'customer retention metrics show', 'NPS fails to capture', 'ML models often amplify', 'AI implementation reality', 'CX automation paradox', 'sarcastically speaking', 'technical reality diverges', 'vendor claims notwithstanding', 'benchmarks contradict', 'despite executive optimism', 'practical implementation shows', 'user testing reveals', 'cynical interpretation suggests', 'empirical evidence contradicts', 'when examined closely'], 'penalty_weight': 0.7, 'reward_weight': 0.6}, 'continuous_training': True, 'checkpoint_interval_seconds': 7200, 'specializations': {'customer_experience': {'keywords': ['customer journey', 'touchpoints', 'engagement', 'satisfaction metrics', 'customer retention', 'experience design', 'CX metrics', 'VoC', 'service design'], 'weight': 0.35}, 'artificial_intelligence': {'keywords': ['machine learning', 'neural networks', 'deep learning', 'LLMs', 'prompt engineering', 'transformer models', 'model training', 'data bias', 'AI ethics'], 'weight': 0.35}, 'machine_learning': {'keywords': ['supervised learning', 'unsupervised learning', 'reinforcement learning', 'model evaluation', 'feature engineering', 'overfitting', 'hyperparameters'], 'weight': 0.3}, 'satirical_style': {'keywords': ['satire', 'irony', 'sarcasm', 'humor', 'mockery', 'parody', 'critique', 'exaggeration', 'wit'], 'weight': 0.5}}}
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_init.py:init():611] starting backend
2025-02-27 17:25:27,532 INFO    MainThread:3239 [wandb_init.py:init():615] setting up manager
2025-02-27 17:25:27,533 INFO    MainThread:3239 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
2025-02-27 17:25:27,535 INFO    MainThread:3239 [wandb_init.py:init():623] backend started and connected
2025-02-27 17:25:27,538 INFO    MainThread:3239 [wandb_init.py:init():715] updated telemetry
2025-02-27 17:25:27,539 INFO    MainThread:3239 [wandb_init.py:init():748] communicating run to backend with 90.0 second timeout
2025-02-27 17:25:27,887 INFO    MainThread:3239 [wandb_run.py:_on_init():2357] communicating current version
2025-02-27 17:25:28,005 INFO    MainThread:3239 [wandb_run.py:_on_init():2366] got version response upgrade_message: "wandb version 0.19.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2025-02-27 17:25:28,005 INFO    MainThread:3239 [wandb_init.py:init():799] starting run threads in backend
2025-02-27 17:25:28,097 INFO    MainThread:3239 [wandb_run.py:_console_start():2335] atexit reg
2025-02-27 17:25:28,097 INFO    MainThread:3239 [wandb_run.py:_redirect():2190] redirect: wrap_raw
2025-02-27 17:25:28,097 INFO    MainThread:3239 [wandb_run.py:_redirect():2255] Wrapping output streams.
2025-02-27 17:25:28,097 INFO    MainThread:3239 [wandb_run.py:_redirect():2280] Redirects installed.
2025-02-27 17:25:28,097 INFO    MainThread:3239 [wandb_init.py:init():842] run started, returning control to user process
2025-02-27 17:25:32,043 INFO    MainThread:3239 [wandb_run.py:_finish():2064] finishing run cpeteconnor-fiverr/pete-connor-cx-ai-expert/aktm9i8g
2025-02-27 17:25:32,043 INFO    MainThread:3239 [wandb_run.py:_atexit_cleanup():2304] got exitcode: 0
2025-02-27 17:25:32,043 INFO    MainThread:3239 [wandb_run.py:_restore():2287] restore
2025-02-27 17:25:32,043 INFO    MainThread:3239 [wandb_run.py:_restore():2293] restore done
