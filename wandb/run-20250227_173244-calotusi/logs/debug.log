2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_setup.py:_flush():76] Current SDK version is 0.16.6
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_setup.py:_flush():76] Configure stats pid to 3364
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_setup.py:_flush():76] Loading settings from /Users/cpconnor/.config/wandb/settings
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_setup.py:_flush():76] Loading settings from /Users/cpconnor/CascadeProjects/multi-platform-content-generator/wandb/settings
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'api_key': '***REDACTED***', 'project': 'pete-connor-cx-ai-expert', 'silent': 'true', 'notebook_name': 'finetune_model.py'}
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'finetune_model.py', 'program_abspath': '/Users/cpconnor/CascadeProjects/multi-platform-content-generator/finetune_model.py', 'program': '/Users/cpconnor/CascadeProjects/multi-platform-content-generator/finetune_model.py'}
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_setup.py:_flush():76] Applying login settings: {}
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_init.py:_log_setup():521] Logging user logs to /Users/cpconnor/CascadeProjects/multi-platform-content-generator/wandb/run-20250227_173244-calotusi/logs/debug.log
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_init.py:_log_setup():522] Logging internal logs to /Users/cpconnor/CascadeProjects/multi-platform-content-generator/wandb/run-20250227_173244-calotusi/logs/debug-internal.log
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_init.py:init():561] calling init triggers
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_init.py:init():568] wandb.init called with sweep_config: {}
config: {'base_model': 'EleutherAI/pythia-1.4b', 'training_config': {'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'training_args': {'output_dir': './outputs/finetune', 'evaluation_strategy': 'steps', 'eval_steps': 100, 'logging_steps': 10, 'save_steps': 7200, 'save_total_limit': 5, 'learning_rate': 2e-05, 'weight_decay': 0.01, 'fp16': False, 'bf16': False, 'max_grad_norm': 0.3, 'max_steps': -1, 'num_train_epochs': 10, 'warmup_ratio': 0.03, 'group_by_length': True, 'lr_scheduler_type': 'cosine', 'report_to': 'wandb', 'gradient_checkpointing': True, 'gradient_accumulation_steps': 16, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'run_name': 'overnight-full-training-run'}}, 'data_config': {'train_file': 'data/training_data.jsonl', 'validation_file': 'data/validation_data.jsonl', 'preprocessing_num_workers': 4, 'max_seq_length': 1024, 'overwrite_cache': False, 'pad_to_max_length': True}, 'wandb_config': {'project': 'pete-connor-cx-ai-expert', 'name': 'overnight-full-training-run', 'tags': ['cx-ai-expert', 'customer-experience', 'machine-learning', 'satirical-tech-expert', 'lora-finetuning', 'continuous-training'], 'notes': "Overnight continuous training run for C. Pete Connor's expertise in customer experience, AI, and ML with enhanced anti-pattern training"}, 'custom_loss_config': {'penalized_phrases': ['game changer', "here's the kicker", 'cutting-edge', 'revolutionary', 'disruptive', 'innovative', 'next-generation', 'state-of-the-art', 'seamless customer journey', 'delightful experience', 'customer-centric', 'AI-powered experience', 'frictionless', 'hyper-personalization', 'digital transformation', 'customer obsession', 'paradigm shift', 'market disruption', 'synergy', 'leverage', 'holistic approach', 'seamless integration', 'robust solution', 'scalable architecture', 'best-in-class', 'end-to-end solution', 'turnkey solution', 'low-hanging fruit', 'think outside the box', 'moving the needle', 'overuse of em dash', 'overuse of en dash', 'artificial transitions', 'symmetric sentence structures'], 'rewarded_phrases': ['paradoxically', 'absurdly', 'statistically speaking', 'data shows', 'ironically', 'contrary to popular belief', 'in stark contrast to the marketing', 'customer experience data indicates', 'model bias reveals', 'sentiment analysis demonstrates', 'customer retention metrics show', 'NPS fails to capture', 'ML models often amplify', 'AI implementation reality', 'CX automation paradox', 'sarcastically speaking', 'technical reality diverges', 'vendor claims notwithstanding', 'benchmarks contradict', 'despite executive optimism', 'practical implementation shows', 'user testing reveals', 'cynical interpretation suggests', 'empirical evidence contradicts', 'when examined closely'], 'penalty_weight': 0.7, 'reward_weight': 0.6}, 'continuous_training': True, 'checkpoint_interval_seconds': 7200, 'specializations': {'customer_experience': {'keywords': ['customer journey', 'touchpoints', 'engagement', 'satisfaction metrics', 'customer retention', 'experience design', 'CX metrics', 'VoC', 'service design'], 'weight': 0.35}, 'artificial_intelligence': {'keywords': ['machine learning', 'neural networks', 'deep learning', 'LLMs', 'prompt engineering', 'transformer models', 'model training', 'data bias', 'AI ethics'], 'weight': 0.35}, 'machine_learning': {'keywords': ['supervised learning', 'unsupervised learning', 'reinforcement learning', 'model evaluation', 'feature engineering', 'overfitting', 'hyperparameters'], 'weight': 0.3}, 'satirical_style': {'keywords': ['satire', 'irony', 'sarcasm', 'humor', 'mockery', 'parody', 'critique', 'exaggeration', 'wit'], 'weight': 0.5}}}
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_init.py:init():611] starting backend
2025-02-27 17:32:44,475 INFO    MainThread:3364 [wandb_init.py:init():615] setting up manager
2025-02-27 17:32:44,476 INFO    MainThread:3364 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
2025-02-27 17:32:44,477 INFO    MainThread:3364 [wandb_init.py:init():623] backend started and connected
2025-02-27 17:32:44,481 INFO    MainThread:3364 [wandb_init.py:init():715] updated telemetry
2025-02-27 17:32:44,481 INFO    MainThread:3364 [wandb_init.py:init():748] communicating run to backend with 90.0 second timeout
2025-02-27 17:32:44,832 INFO    MainThread:3364 [wandb_run.py:_on_init():2357] communicating current version
2025-02-27 17:32:44,943 INFO    MainThread:3364 [wandb_run.py:_on_init():2366] got version response upgrade_message: "wandb version 0.19.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2025-02-27 17:32:44,943 INFO    MainThread:3364 [wandb_init.py:init():799] starting run threads in backend
2025-02-27 17:32:45,023 INFO    MainThread:3364 [wandb_run.py:_console_start():2335] atexit reg
2025-02-27 17:32:45,023 INFO    MainThread:3364 [wandb_run.py:_redirect():2190] redirect: wrap_raw
2025-02-27 17:32:45,023 INFO    MainThread:3364 [wandb_run.py:_redirect():2255] Wrapping output streams.
2025-02-27 17:32:45,023 INFO    MainThread:3364 [wandb_run.py:_redirect():2280] Redirects installed.
2025-02-27 17:32:45,023 INFO    MainThread:3364 [wandb_init.py:init():842] run started, returning control to user process
2025-02-27 17:32:51,311 INFO    MainThread:3364 [wandb_run.py:_config_callback():1347] config_cb None None {'peft_config': {'default': {'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, 'peft_type': <PeftType.LORA: 'LORA'>, 'auto_mapping': None, 'base_model_name_or_path': 'EleutherAI/pythia-1.4b', 'revision': None, 'inference_mode': False, 'r': 8, 'target_modules': {'dense_h_to_4h', 'query_key_value', 'dense_4h_to_h', 'dense'}, 'exclude_modules': None, 'lora_alpha': 16, 'lora_dropout': 0.05, 'fan_in_fan_out': False, 'bias': 'none', 'use_rslora': False, 'modules_to_save': None, 'init_lora_weights': True, 'layers_to_transform': None, 'layers_pattern': None, 'rank_pattern': {}, 'alpha_pattern': {}, 'megatron_config': None, 'megatron_core': 'megatron.core', 'loftq_config': {}, 'eva_config': None, 'use_dora': False, 'layer_replication': None, 'runtime_config': {'ephemeral_gpu_offload': False}, 'lora_bias': False}}, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float32', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['GPTNeoXForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': None, 'eos_token_id': 0, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'EleutherAI/pythia-1.4b', '_attn_implementation_autoset': True, 'transformers_version': '4.49.0', 'model_type': 'gpt_neox', 'vocab_size': 50304, 'max_position_embeddings': 2048, 'hidden_size': 2048, 'num_hidden_layers': 24, 'num_attention_heads': 16, 'intermediate_size': 8192, 'hidden_act': 'gelu', 'rotary_pct': 0.25, 'partial_rotary_factor': 0.25, 'rotary_emb_base': 10000, 'rope_theta': 10000, 'attention_dropout': 0.0, 'hidden_dropout': 0.0, 'classifier_dropout': 0.1, 'initializer_range': 0.02, 'layer_norm_eps': 1e-05, 'use_cache': True, 'use_parallel_residual': True, 'rope_scaling': None, 'attention_bias': True, 'output_dir': './outputs/finetune', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': True, 'do_predict': False, 'eval_strategy': 'steps', 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 16, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 0.3, 'num_train_epochs': 10, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.03, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': './outputs/finetune/runs/Feb27_17-32-46_Mac.lan', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': 'steps', 'save_steps': 7200, 'save_total_limit': 5, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 100, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'overnight-full-training-run', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_torch', 'optim_args': None, 'adafactor': False, 'group_by_length': True, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': 'outputs/finetune/checkpoint-50', 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': None, 'hub_always_push': False, 'gradient_checkpointing': True, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'evaluation_strategy': 'steps', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'dispatch_batches': None, 'split_batches': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'eval_use_gather_object': False, 'average_tokens_across_devices': False}
2025-02-27 17:32:51,312 INFO    MainThread:3364 [wandb_config.py:__setitem__():151] config set model/num_parameters = 1420939264 - <bound method Run._config_callback of <wandb.sdk.wandb_run.Run object at 0x16b04fa10>>
2025-02-27 17:32:51,312 INFO    MainThread:3364 [wandb_run.py:_config_callback():1347] config_cb model/num_parameters 1420939264 None
2025-02-27 17:32:52,801 INFO    MainThread:3364 [wandb_run.py:_finish():2064] finishing run cpeteconnor-fiverr/pete-connor-cx-ai-expert/calotusi
2025-02-27 17:32:52,801 INFO    MainThread:3364 [wandb_run.py:_atexit_cleanup():2304] got exitcode: 0
2025-02-27 17:32:52,801 INFO    MainThread:3364 [wandb_run.py:_restore():2287] restore
2025-02-27 17:32:52,801 INFO    MainThread:3364 [wandb_run.py:_restore():2293] restore done
