2025-02-26 21:04:12,805 INFO    MainThread:8872 [wandb_setup.py:_flush():76] Current SDK version is 0.16.6
2025-02-26 21:04:12,805 INFO    MainThread:8872 [wandb_setup.py:_flush():76] Configure stats pid to 8872
2025-02-26 21:04:12,805 INFO    MainThread:8872 [wandb_setup.py:_flush():76] Loading settings from /Users/cpconnor/.config/wandb/settings
2025-02-26 21:04:12,805 INFO    MainThread:8872 [wandb_setup.py:_flush():76] Loading settings from /Users/cpconnor/CascadeProjects/multi-platform-content-generator/wandb/settings
2025-02-26 21:04:12,805 INFO    MainThread:8872 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'mode': 'offline'}
2025-02-26 21:04:12,805 INFO    MainThread:8872 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2025-02-26 21:04:12,806 INFO    MainThread:8872 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': 'finetune_model.py', 'program_abspath': '/Users/cpconnor/CascadeProjects/multi-platform-content-generator/finetune_model.py', 'program': '/Users/cpconnor/CascadeProjects/multi-platform-content-generator/finetune_model.py'}
2025-02-26 21:04:12,806 INFO    MainThread:8872 [wandb_init.py:_log_setup():521] Logging user logs to /Users/cpconnor/CascadeProjects/multi-platform-content-generator/wandb/offline-run-20250226_210412-w0bz4njw/logs/debug.log
2025-02-26 21:04:12,806 INFO    MainThread:8872 [wandb_init.py:_log_setup():522] Logging internal logs to /Users/cpconnor/CascadeProjects/multi-platform-content-generator/wandb/offline-run-20250226_210412-w0bz4njw/logs/debug-internal.log
2025-02-26 21:04:12,806 INFO    MainThread:8872 [wandb_init.py:init():561] calling init triggers
2025-02-26 21:04:12,806 INFO    MainThread:8872 [wandb_init.py:init():568] wandb.init called with sweep_config: {}
config: {'base_model': 'EleutherAI/pythia-1.4b', 'training_config': {'lora_r': 16, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'training_args': {'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'warmup_steps': 100, 'max_steps': 1000, 'learning_rate': 0.0003, 'fp16': True, 'logging_steps': 10, 'output_dir': './outputs/finetune', 'save_strategy': 'steps', 'save_steps': 200, 'evaluation_strategy': 'steps', 'eval_steps': 200, 'report_to': 'wandb'}}, 'data_config': {'train_file': 'data/training/pete_connor_style.jsonl', 'validation_file': 'data/training/pete_connor_validation.jsonl', 'preprocessing_num_workers': 4, 'max_seq_length': 512, 'overwrite_cache': False, 'pad_to_max_length': True}, 'wandb_config': {'project': 'pete-connor-cx-ai-expert', 'name': 'cx-ai-ml-expertise-run', 'tags': ['cx-ai-expert', 'customer-experience', 'machine-learning', 'satirical-tech-expert', 'lora-finetuning'], 'notes': "Fine-tuning run for C. Pete Connor's expertise in customer experience, AI, and machine learning"}, 'custom_loss_config': {'penalized_phrases': ['game changer', "here's the kicker", 'cutting-edge', 'revolutionary', 'disruptive', 'innovative', 'next-generation', 'state-of-the-art', 'seamless customer journey', 'delightful experience', 'customer-centric', 'AI-powered experience', 'frictionless', 'hyper-personalization', 'digital transformation', 'customer obsession'], 'rewarded_phrases': ['paradoxically', 'absurdly', 'statistically speaking', 'data shows', 'ironically', 'contrary to popular belief', 'in stark contrast to the marketing', 'customer experience data indicates', 'model bias reveals', 'sentiment analysis demonstrates', 'customer retention metrics show', 'NPS fails to capture', 'ML models often amplify', 'AI implementation reality', 'CX automation paradox'], 'penalty_weight': 0.5, 'reward_weight': 0.5}}
2025-02-26 21:04:12,806 INFO    MainThread:8872 [wandb_init.py:init():611] starting backend
2025-02-26 21:04:12,806 INFO    MainThread:8872 [wandb_init.py:init():615] setting up manager
2025-02-26 21:04:12,806 INFO    MainThread:8872 [backend.py:_multiprocessing_setup():105] multiprocessing start_methods=spawn,fork,forkserver, using: spawn
2025-02-26 21:04:12,807 INFO    MainThread:8872 [wandb_init.py:init():623] backend started and connected
2025-02-26 21:04:12,809 INFO    MainThread:8872 [wandb_init.py:init():715] updated telemetry
2025-02-26 21:04:12,809 INFO    MainThread:8872 [wandb_init.py:init():748] communicating run to backend with 90.0 second timeout
2025-02-26 21:04:12,811 INFO    MainThread:8872 [wandb_init.py:init():799] starting run threads in backend
2025-02-26 21:04:12,859 INFO    MainThread:8872 [wandb_run.py:_console_start():2335] atexit reg
2025-02-26 21:04:12,859 INFO    MainThread:8872 [wandb_run.py:_redirect():2190] redirect: wrap_raw
2025-02-26 21:04:12,860 INFO    MainThread:8872 [wandb_run.py:_redirect():2255] Wrapping output streams.
2025-02-26 21:04:12,860 INFO    MainThread:8872 [wandb_run.py:_redirect():2280] Redirects installed.
2025-02-26 21:04:12,860 INFO    MainThread:8872 [wandb_init.py:init():842] run started, returning control to user process
2025-02-26 21:07:18,529 WARNING MsgRouterThr:8872 [router.py:message_loop():77] message_loop has been closed
